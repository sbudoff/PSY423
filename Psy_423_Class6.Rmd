---
title: "PSY 423 Class 6"
author: "S. Budoff"
date: "2023-09-11"
output: ioslides_presentation
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(warning = FALSE, message = FALSE, echo = FALSE)
```

```{r load}
library(tidyverse)
library(googlesheets4)
library(patchwork)
library(qrcode)

gs4_auth(email = "sabudoff@gmail.com")

#Sort the integer values
coffee_levels <- c("Love it!", "It's good", "No opinion", "It's bad", "Ewwww!")
shoe_levels <- c("M 0-4.5 / W 0-6",
                 "M 5-6 / W 6.5-7.5",
                 "M 6.5-7.5 / W 8-9",
                 "M 8-9 / W 9.5-10.5",
                 "M 9.5-10.5 / W 11-12",
                 "M 11-12 / W 12.5-13.5",
                 "M 12.5+ / W14+")

url = "https://docs.google.com/spreadsheets/d/18Vj9uuD51ZHRds2Jqve91Cii_x46lJJl5ErrEwb4tKs/edit#gid=976217741"
df <- read_sheet(url, sheet = "Form Responses 1")
df <- df %>%
  mutate(Shoe_Size = factor(`What size shoe do you wear?`, levels=shoe_levels),
         HairColor = factor(`What color is your hair right now?`),
         Height = `How tall are you (in inches)?`,
         Coffee = factor(`How much do you like coffee?`, levels=coffee_levels)) %>%
  select(Shoe_Size, HairColor, Height, Coffee)
```

## Warmup Quiz

```{r qr}
code_link <- "https://forms.gle/gnhCP36HpQuTmnwa7"
plot(qr_code(code_link))
print(code_link)
```


## Confidence Intervals

A set/interval of numbers that contain the parameter we care about a specified percent of time

Usually 95%

If the parameter we care about is the mean, in frequentist statistics it is the interval we predict to contain the sample mean for 95 out of 100 experiments we conduct


## How do you think sample size will relate to confidence intervals?

```{r std}
# Create a data frame with different sample sizes
sample_sizes <- seq(2, 200, by = 10)
data <- data.frame(sample_size = sample_sizes)

# Generate means and standard errors for each sample size
data$mean <- sapply(sample_sizes, function(n) {
  x <- rnorm(n)  # Generate random data
  mean(x)
})

data$std_error <- sapply(sample_sizes, function(n) {
  x <- rnorm(n)  # Generate random data
  sd(x) / sqrt(n)  # Calculate standard error
})

# Create a plot
ggplot(data, aes(x = sample_size, y = mean, ymin = mean - 1.96 * std_error, ymax = mean + 1.96 * std_error)) +
  geom_point() +
  geom_errorbar(width = 0.2) +
  labs(
    x = "Sample Size",
    y = "Mean",
    title = "Effect of Sample Size on Confidence Interval Width"
  ) +
  theme_minimal()
```

## N's effect on CIs come from the standard error of the mean

Increasing the sample size decreases the width of confidence intervals, because it decreases the standard error.

Standard error (SE or SEM or $\sigma_{\bar{x}}$) of estimate is a statistic of the stability and reliability of data. It is the variability across MULTIPLE samples

Standard error is used to estimate the population standard deviation, which is the variability across a population, or in the case of a sample standard deviation across a SINGLE sample.

standard error:
$$\sigma_{\bar{x}} = \frac{s}{\sqrt{n}}$$

## How is the 95% CI computed

$$CI = 1.96 * \sigma_{\bar{x}} = 1.96 *\frac{s}{\sqrt{n}}$$

Where does 1.96 come from?

How can we relate this equation to the Z-distribution?

## Revisitting the Z test

We originally described the sample calculation as $$z = \frac{x - \bar{x}}{s} $$ 

Where you first use your raw data to calculate your sample mean ($\bar{x}$), and sample standard deviation ($s$), then you can use these values to standardize your value of interest ($x$).

Because  with the theoretical population, the logic is the same but the symbols are now $\mu$ and $\sigma$ for the population mean and standard deviation respectively, so the formula looks like: $$z = \frac{x - \mu}{\sigma}$$

However, in practice this is not true

## When to use the Z-test

## When to use the Z-test

1) Our data is a continuous variable 

2) Our data represents a random sample from an identical distribution

3) We gathered our samples independently of one another

4) We know the standard deviation for the population

5) We believe our data is normally distributed (at least at the limit)

In practice the last condition is met when
$$n \geq 30$$

## In your groups

Can you think of examples where this is not applicable?

What is the problem with using $s$ in place of $\sigma$?

How would you correct the Z-test?
$$z = \frac{x - \mu}{\sigma}$$


## How would you correct the Z-test

We can use SEM ($\sigma_{\bar{x}}$)

So in practice we really must compute

$$"z" = \frac{x - \bar{x}}{\sigma_{\bar{x}}} = \frac{x - \bar{x}}{\frac{s}{\sqrt{n}}} $$ 

## Small Sample Sizes are very common

William Sealy Gosset taught us to consider the sample standard deviation in 1908 when he faced this challened

![](/home/sam/Regis/PSY423/gosset.jpg){width=800px}

## The real distributions are thus

When $sigma$ is known
$$z = \frac{x - \mu}{\sigma} \implies \frac{x - \bar{x}}{\sigma} $$ 

When $sigma$ is unknown

$$t = \frac{x - \mu}{s_{\bar{x}}} \implies \frac{x - \bar{x}}{s_{\bar{x}}} = \frac{x - \bar{x}}{\frac{s}{\sqrt{n}}} $$ 

## Degrees of freedom, one last idea

In many distributions, including the t-distribution, we accept that we are dealing with uncertainty

We specifically say we are uncertain about our distribution in relation to how many samples we have

Mathematically we call this degrees of freedom (df)

$$df = n-1$$

Why minus 1? Because the mean is estimated, and thus a fixed value

## Visualizing the t-distributions

```{r tdists}

# Generate a sequence of x values
x <- seq(-4, 4, by = 0.01)
dfs <- c(1, 3, 10, 30)
# Create a data frame with x values and PDF values for different degrees of freedom
df <- data.frame(x = rep(x, times = length(dfs)), 
                 df = rep(dfs, each = length(x)),
                 pdf = dt(x, df = rep(dfs, each = length(x))))

# Create the plot
ggplot(df, aes(x = x, y = pdf, color = as.factor(df))) +
  geom_line() +
  geom_line(data = data.frame(x = x, pdf = dnorm(x)), aes(x = x, y = pdf), linetype = "dashed", color = "black") + # Add the standard normal PDF
  labs(title = "T-Distribution PDF with Different Degrees of Freedom", y = "Probability", x="standard deviation") +
  scale_color_discrete(name = "Degrees of Freedom") +
  theme_minimal() 
```

In your groups try to describe it in words and what is interesting about it

## When to use the t-distribution


1) Our data is a continuous variable 

2) Our data represents a random sample from an identical distribution

3) We gathered our samples independently of one another

4) We believe our data is normally distributed (at least at the limit)

<s>5) We know the standard deviation for the population</s>


## Typese of t-tests

1) Single Sample

2) Independent Samples

3) Paired Samples (AKA: Dependent Samples)

## Single Sample

Used when we have a predicted mean ($\mu_0$) we would like to assess a single sample distribution from

Think of an experiment where this is applicable

Come up with the null and alternative hypothesis

What will be the degrees of freedom?

Try to write the test equation


## Single Sample

Used when we have a predicted mean ($\mu_0$) we would like to assess a single sample distribution from

$$H_0 : \bar{x} = \mu_0$$

$$H_a : \bar{x} \neq \mu_0$$

$$df = n-1$$

$$ t = \frac{\bar{x} - \mu_0}{s_{\bar{x}}} = \frac{\bar{x} - \mu_0}{\frac{s}{\sqrt{n}}}$$


## Independent Samples

Used when we have two samples and we would like to predict if they come from the same distribution

Think of an experiment where this is applicable

Come up with the null and alternative hypothesis

What is funny about the sample size now? Can you think of a math trick we have used before to deal with it?

What will be the degrees of freedom?

Try to write the test equation


## Independent Samples

Used when we have a predicted mean ($\mu_0$) we would like to assess a single sample distribution from

$$H_0 : \bar{X}_1 = \bar{X}_2 \text{   OR   } \mu_1 = \mu_2$$

$$H_a : \bar{X}_1 \neq \bar{X}_2  \text{   OR   } \mu_1 \neq \mu_2$$

$$df = n_1 + n_2 - 2$$

$$ t = \frac{\bar{X}_1 - \bar{X}_2}{SEM_{pooled}} = \frac{\bar{X}_1 - \bar{X}_2}{s_{pooled}\sqrt{\frac{1}{n_1}+\frac{1}{n_2}}} $$

## Paired Samples

Used when we have two samples that are matched in some way and we would like to predict if their distribution has changed

Think of an experiment where this is applicable

Can you think of a math trick we have used before to see if there is a difference between pairs?

Come up with the null and alternative hypothesis

What will be the degrees of freedom?

Try to write the test equation


## Paired Samples

Used when we have a predicted mean ($\mu_0$) we would like to assess a single sample distribution from

$$H_0 : \text{Mean}(X_1-X_2) = \bar{d} = 0 \text{    OR    } \mu_1 - \mu_2 = \mu_d = 0$$

$$H_a : \text{Mean}(X_1-X_2) = \bar{d} \neq 0 \text{    OR    } \mu_1 - \mu_2 = \mu_d \neq 0$$

$$df = n_1 - 1$$

$$ t = \frac{\bar{d} - 0}{s_{\bar{d}}} = \frac{\bar{d}}{\frac{s_d}{\sqrt{n}}}$$

